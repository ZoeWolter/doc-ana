<!DOCTYPE html>
<html>
<head>
    <title>DocAna Project 2023</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0" charset="utf-8">
    <script src="d3.js" charset="utf-8"></script>
    <link rel="stylesheet" href="index.css">
</head>

<body>
  <fieldset id="header">
  <div>
    <h2>From Male to Female Perspectives: Comparative Topic Analysis of AskWomen and AskMen Subreddits</h2>
    <p>Daniela Blumberg, Nele Hapig and Zoé Wolter</p>
  </div>
  </fieldset>
  <fieldset id="intro">
    <div>
      <img class="responsive-image"  src="img/AskWomen_WordCloud.png" alt="AskMen WordCloud">
    </div>
    <div>
      <br>
      <br>
      <p class="bigger">
        <b>Are people interested in different topics when it comes to asking either men or women?</b>
        <br><br>
        The AskWomen and AskMen subreddits serve as virtual spaces where individuals share their thoughts, seek advice and engage in conversations. 
        <br><br>
        By analyzing the topics discussed within these communities, we can explore how gender influences the subjects of interest and the nature of interactions.
        <br>
        Thus, the goal of this project is to uncover shared topics of interest, unique concerns and potential areas of convergence between male and female communities.
      </p>
    </div>
    <div>
      <img class="responsive-image" src="img/AskMen_WordCloud.png" alt="AskWomen WordCloud">
    </div>
  </fieldset>

  <fieldset class="highlighted">
    <div class="section">
      <h3>Context</h3>
    </div>
  </fieldset>

  <fieldset>
    <div>
      <p>
        The AskMen and AskWomen subreddits are already used for other analyses, e.g. by Zayats and Ostendorf (2018) 
        who evaluate an approach to predict the <b>popularity of comments</b> in threaded discussions on social media. 
        Furthermore, another analysis investigates <b>withdrawal-related Reddit posts</b> in those two subreddits and finds
        large gendered variation (cf. Latack et al. (2021)). Jaech et al. (2015) ask how <b>language affects the reaction</b> of
        community in Reddit comments and find that informativeness and relevance are useful feature categories to
        improve comment ranking. On top of that, text, users, and sentiment can be included in order
        to research gendered communities (cf. Lucy and Mendelsohn (2018)). This study shows that "the high <b>sentiment 
        similarity</b> between r/askmen and r/askwomen misaligns with their low <b>text similarity</b> [...] and near
        average <b>user similarity</b>". Khan and Golab (2020) aim to understand gendered movements on Reddit and thus use topic
        modelling (Non-negative Matrix Factorization topic modelling algorithm). They conclude that the subreddits 
        AskMen and AskWomen are <b>rather similar</b> compared to other subreddits. 
        <br><br>
        Our analysis builds on this exisiting literature by analyzing the topics which are discussed in both
        subreddits. The description of the subreddits already indicates that <b>there might be differences in the topics</b>, 
        but can we confirm this with our analysis?
        <ul>
          <li><p><b>AskWomen:</b> A subreddit dedicated to asking women questions about their thoughts, lives, and experiences.</p></li>
          <li><p><b>AskMen:</b> A semi-serious place to ask men casual questions about life, career, and more.</p></li>
        </ul>
      </p>
    </div>
  </fieldset>

  <fieldset class="highlighted">
    <div class="section">
      <h3>Data & Preprocessing</h3>
    </div>
  </fieldset>

  <fieldset>
    <div>
      <p>
        From the whole reddit dataset, we started to filter the data to get the posts written online in the AskMen and AskWomen 
        subreddits.
        In total we got 14538 posts in the AskMen and 9278 posts in the AskWomen community. 
        This inbalance suggests that the AskMen community is more popular and more frequently used, 
        which also rose the question if there are other significant <strong>differences between the communities</strong> apart 
        from the contents discussed.
        <br><br>
        One thing we looked at is the average post length in both subreddits. 
        With an average post length of 257 for the AskMen, and 250 for the AnskWomen subreddit, there is no large difference visible. 
        The following bar plot shows the distribution of post lengths within both subreddits:
      </p>
        <div style="margin:0 auto;text-align:center">
          <img class="responsive-image" src="img/post-length.svg" alt="Distribution post lengths">
        </div>
      <p>  
        Prior to conducting the analysis, we applied several <strong>preprocessing</strong> steps to the raw data obtained from the AskMen and AskWomen subreddits.
        Firstly, all the text was converted to lowercase to achieve case insensitivity. 
        To account for negation short forms, such as "can't" or "won't,"  we removed apostrophes from the posts. 
        Next, we removed special characters to filter out any non-essential symbols or punctuation marks.
        Finally, we applied lemmatization to reduce words to their base form, ensuring that words with similar meanings are treated equally.
        By implementing these preprocessing steps, the raw text data from the AskMen and AskWomen subreddits were transformed into a standardized and cleaner format, 
        laying the foundation for the subsequent topic analysis.
        The preprocessed data was also used to create the wordclouds displayed above. However, stopwords were removed from the text as well for creating the wordclouds. 
        For BERTopic it is suggested to not apply stopword removal to the data input before generating the model. 
        Even though stopwords do not carry strong semantic meaning, they can still be important for the context of the other words in a post.
        Therefore, the data for the following BERTopic model was preprocessed with the steps preprocessed above, but no stopwords were removed. 
      </p>
    </div>
  </fieldset>
  <fieldset class="highlighted">
    <div>
      <h3>Topic Analysis using BERTopic</h3>
    </div>
  </fieldset>
  <fieldset>
    <div>
      <p>
        For a topic analysis, we used the topic modeling technique of <a href="https://github.com/MaartenGr/BERTopic">BERTopic</a>. 
        BERTopic does not provide a method that allows for the direct comparison of the topics of two groups, which would be needed to compare the topics in the two subreddits. 
        Theoretical, one could perform BERTopic individually on each dataset, but one may end up with different topic models for each dataset. 
        This would make it challenging to compare and contrast the topics directly. 
        By merging the datasets and applying BERTopic to all posts at once, one can ensure that the same topic modeling approach is applied uniformly to both datasets, enabling direct comparison of shared topics and differences.
        Therefore, we merged the data of both subreddits 'AskMen' and 'AskWomen', leading to 23816 reddit posts overall as input data 
        to BERTopic. The <strong>BERTopic's algorithm</strong> comprises five steps:
        <br><br>
        1. <strong>Embeddings:</strong> Reddit posts are transformed to numerical representations <br>
        2. <strong>Dimensionality Reduction:</strong> here the Uniform Manifold Approximation and Projection (<a href="https://github.com/lmcinnes/umap">UMAP</a>) 
        dimension reduction technique is used<br>
        3. <strong>Clustering:</strong> the hierarchical clustering algorithm HDBSCAN is used in order to cluster the posts <br>
        4. <strong>Tokenizer:</strong> using a bag-of-words representation of each cluster, the most frequent words of each cluster are defined <br>
        5. <strong>Topic representation:</strong> using a class-based TF-IDF as weighting scheme, the algorithm returns the "most important" words of 
        each cluster which represent the topic, i.e. these words constitute the topic name
        <br><br>
        When applying the BERTopic model to our reddit data, each post is assigned to exactly one topic, where each topic is represented as unique ID followed by its 4 most representive words. 
        Additionally, we get the information if a certain post is representative for the assigned topic or not.
        Then, for each topic, we extract the absolute number of posts assigned to the topic, its representative posts and the top 10 most important words.
        <br><br>
        In BERTopic, the topic ID -1 is used to represent outliers or documents that do not strongly align with any particular topic.
        In our case, we got 11361 reddit posts that cannot be matched to a topic cluster and are thus classified as topic -1. 
        These outliers make up 48.6% of the AskMen and 45.9% of the AskWomen posts, corresponding to overall 47.7% of our data. 
        This is a significant portion of posts that do not fit into any specific topic cluster determined by the algorithm. 
        This could be due to several reasons. It's possible that a substantial portion of the posts contain irrelevant content or diverse topics that do not form coherent clusters. 
        Another explaination could be the heterogeneity in our data due to the diverse sources writing posts and due to the wide range of topics that could be covered.
        In our case, the topic -1 corresponds to 'relationship_be_girl_her', so there seems to be at least some common intuition (girls in relationships) inside the cluster.
        Nevertheless, we decided to exclude the posts classified as -1 from our further analysis to enhance the interpretability and to focus on meaningful topics.
        In line with this, we also restricted our analysis to the top 50 topics in terms of overall occurence.
        <br><br>
        The BERTopic model offers in-build visualization that we used to get a first impression of the data. 
        The conclusions drawn from the visualizations are discussed below. 
      </p>
    </div>
  </fieldset>
  <fieldset>
    <iframe src="img/fig_combined_documents.html" width="100%" height="800px"></iframe>
  </fieldset>

  <fieldset>
    <p>
      The above diagram shows all posts as individual circles with a position given by their 2-dimensional embeddings. 
      The color of each circle reflects the assigned topic of the respective post. 
      One can direclty see the huge number of unclassified posts spread all over the visualization, displayed as greyish circles.
      Moreover, the figure gives a first insight into the relationship between the individual posts and the topics. 
      Posts that belong to the same topic tend to be close to each other and form dense clusters, as in the case of topic 8 or 9. 
      However, some topics seem to be more spread out as for instance topic 6, 12 or 20.
      Some contributions belonging to a certain topic are very close to other themes (see topic 5 about parents and 7 about career). 
      These posts and topics seem to be more closely related in terms of semantics to each other.
      In contrast, some topics are farther apart from all the others or at the corners (see topic 32 about tattoos and piercings). 
      Moreover, one can also see that some topics appear more frequent than others. 
      An example here is given by topic 0, that takes up almost the entire upper left corner.
      Topic number 47 seems to be rather an outlier topic as it is diplayed far apart from the others in the upper right corner and does not seem to have much posts assigned to it. 
      The name seems also strange, which strengthens this assumption.
      Althought the visualization includes an option to filter the data (hide certain topics), it is very crowded and suffers heavily from overplotting, making it hard to draw conclusions on individual topics or posts.
      With an increased number of topic, it is also difficult to distinguish the topics from each other, as the same color is used for multiple topics.
    </p>
  </fieldset>

  <fieldset>
    <iframe src="img/fig_combined_barchart.html" width="100%" height="800px" frameborder = "0"></iframe>
    <iframe src="img/fig_combined_heatmap.html" width="100%" height="800px"></iframe>
  </fieldset>  

  <fieldset>
    <p>
      The figure on the left shows the top 20 topics derived from the BERTopic model and illustrates the relative c-TF-IDF scores of the most frequently recurring words associated with each topic.
      The score for the most occuring words in these topics seems to be on average around 0.4. However, some words seem to carry more importance for a topic than others.
      For example, the word 'confidence' seems to be more likely to occur in topic 1 than 'social' or 'talk'. 
      On the other hand, in the case for example topic 14 or 18, the top 5 words seem to be occur approximaltely equally likely.
      Using this plot, we can also make inferences about the semantics of each topic. 
      For example, topic 18 seems to clearly discuss issues related to food and cooking, topic 9 is about marriages or engagements and topic 5 is about family.
      <br><br>
      In addition to the c-TF-IDF scores for each word of a topic, BERTopic also calculates topic embeddings. 
      These are the weighted average of the c-TF-IDF scores of the words that belong to each topic. Based on these topic embeddings, 
      we can create a matrix by applying cosine similarities, calculated as the cosine of the angle between two vectors. 
      The figure on the right shows the resulting similarity matrix of the top 50 topics. As we can see, the first topics created by the model, 
      that also contain most of the posts, are more similar to each other than the last topics. This indicates that the topics discussed 
      in the majority of posts can be distinguished from each other but are in general related in their content. 
      Looking at the names of the topics and the most occuring words, this makes sense as many topics seem to be about relationship, love, sex, dating etc..
      
    </p>
  </fieldset>

  <fieldset>
    <iframe src="img/fig_combined_topics.html" width="100%" height="960px"></iframe>
    <iframe src="img/fig_combined_hierarchy.html" width="150%" height="960px"></iframe>
  </fieldset>

  <fieldset>
    <p>
     BERTopic also performs clustering and provides a visualization of the resulting topic clusters.   
     One can clearly see in this visualization that some topics seem to be closely related and form dense clusters, while being well separated from the other topics.
     Additionally, one can see that topic 47 actually seems to be an outlier as suspected, because it is merged at the latest stage in the dendogram on the right.
     Despite these insights, we can still make no conclusions about which topic is more or less discussed in either the AskMen or AskWomen subreddit.
     <br><br>
     In conclusion, the build-in visualizations of BERTopic offer some insights into the individual posts, the extracted topics and their similarities.
     However, they do not provide a way to compare differences between the AskMen and AskWomen community.
     To compare the subreddits in terms of shared and unique topics of interest, we needed to come up with our own
     visualizations that address our research question.
    </p>
  </fieldset>
  
  <fieldset class="highlighted">
    <div>
      <h3>Visual comparison of the most discussed topics</h3>
    </div>
  </fieldset>
  <fieldset>
    <div>
      <p>
        For a first analysis of the topics and whether they are more likely to be discussed in the 'AskMen' or 
        'AskWomen' subreddit, we decided to look at a simple bar chart for the 10 topics with the highest occurence in both subreddits.
        <br> <br>
        <strong>Adjustment for imbalance in the dataset:</strong>
        <br>
        To calculate the occurence of each topic relative to the AskMen and AskWomen subreddits, we needed to take the inbalance of our dataset into account.
        Using the absolute number of posts within a topic for AskMen and AskWomen (topic_count_men, topic_count_women) and having 14538 posts in total in AskMen and 9278 in AskWomen, the topic share for each subreddit is given by:
        <br>
        <code>topic_share_men = (topic_count_men / 14538) / (topic_count_men / 14538 + topic_count_women / 9278)</code>
        <br><br>
        <code>topic_share_women = (topic_count_women / 9278) / (topic_count_women / 9278 + topic_count_men / 14538)</code>
        <br><br>
        Using these formulas, topic_share_men and topic_share_women always add up to 1, such that one can easily compare the relative occurences.
      </p>
    </div>
  </fieldset> 

  <div style="margin:0 auto;text-align:center">
    <object data="img/top10_shares_adjusted.svg"></object>
  </div>
  <fieldset>
    <div>
      <p>
        One can see that there are actually topics that are more likely to be discussed 
        in 'AskMen' such as <i>'college_be_carreer_live'</i>, whereas others are more likely to be addressed in 'AskWomen'
        as for instance <i>'shave_shaving_hair_shaved'</i> or <i>'wedding_marriage_engagement_bride</i>.  
        However, many topics seem to be discussed approximately equally often - at least for the displayed top 10 topics. 
        Additionally to showing only the top 10 topic, this kind of visualization does not include any information about the overall importance of a topic and the similarity between topics.
        Furthermore, it does not scale well if we want to look at more than the top 10 topics. 
        Going deeper into the analyis of the top 50 topics, this approach seems to be not expressive enough to completely answer our reasearch question.
        Thus, it was our motivation to come up with an advanced visualization that adressed these limitations and that is even able to include more information 
        about each topic through user interactions, i.e. a tooltip displayed when hovering over a topic of interest.
      </p>
    </div>
  </fieldset> 

  <fieldset>
    <div>
      <p>
        The implementation of our interactive visulaization included several steps that are explained in the following. <br><br>
        <strong>Topic embeddings:</strong>
        <br>
        From the BERTopic model, we extracted high dimensional embeddings, capturing the semantic information and meaning of each topic. 
        To be able to effectively visualize the topics, we aimed to obtain a two-dimensional representation of the topic embeddings.
        In doing so, we applied <strong><a href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html">
        Principal Component Analysis (PCA)</a></strong>, a dimensionality reduction technique that aims to capture the maximum amount of variation present in the data, 
        while projecting them onto a lower-dimensional space. 
        This allows for a more concise representation approximately respresenting the underlying topic structure and preserving key patterns and relationships between topics.
        Consequently, topics that are semantically similar or closely related should be positioned closer together in the 2D space, while those that are dissimilar should be placed farther apart. 
        This arrangement allows to identify clusters, patterns, and even potential topic hierarchies within the dataset.
        <br><br>
        BERTopic itself does also perform dimensionality reduction on the embeddings to produce some in-build visualizations but unfortunately
        the model does not provide an option to output the embeddings directly in a lower dimensionality.
        <br>
        An alternative idea was also to calculate the topic embeddings using static word embeddings extracted from the Glove 200 embeddings trained on a large corpus of twitter data. 
        More specifically, one could get a word embedding for each word (that is not out of vocabulary) in a topic's name and then take the mean of the word embeddings as overall topic embeddings.
        Taking the mean should work quite well in this scenario since the words constituting a topic's name should be close together in the embedding space.
        We actually tried this approach as well and the results were quite similar to those obtained above but as we expect the embeddings coming from BERTopic to be more precise,
        we decided to stick with this procedure.
        <br><br>
        <strong>Design decisions:</strong>
        <br>
        We decided to represent each topic by a circle varying in size and color. The <strong>position</strong> of each cirlce is given by the <strong>2D embeddings</strong>.
        An alternative to use circles to represent the topics would be to plot the topic names directly. 
        However, with an increasing number of topics to be analyzed, this would result in severe overplotting. This issue is significantly reduced by taking circles instead.
        <br>
        The <strong>size</strong> of each circle corresponds to the <strong>overall occurrence</strong> of the respective topic in the dataset. 
        Larger circles indicate topics that appear more frequently, while smaller circles represented less common topics. 
        This size-based representation offers an intuitive overview of the importance and prevalence of each topic in the overall dataset. 
        To ensure that each topic is visible in the visualization, we ordered the topics accordingly and draw small circles on top of larger ones.
        <br><br>
        We used the <strong>color</strong> of the circles to analyze the distribution of topics across the subreddits.
        Topics predominantly discussed in the <strong>AskMen</strong> subreddit are represented by <strong>blue</strong> circles,
        while topics more commonly found in <strong>AskWomen</strong> posts are depicted with <strong>red</strong> circles. 
        Topics with a similar level of occurrence in both subreddits are mapped to a grey color. 
        This color-sceme offers an immediate visual cue to the relative prevalence of a topic in each subreddit. 
        By examining the distribution of blue and red, one can quickly identify topics that are more popular or relevant to one gender-specific community compared to the other. 
        <br>
        In order to identify each topic, we added a <strong>tooltip</strong>, displaying the most relevant information about each topic. 
        This includes the topics name, its id, the top 10 occuring words, the occurence of the topic as absolute number and the respective percentage for AskMen and AskWomen occurences.
        <br>
      </p>
    </div>
  </fieldset>
  <fieldset id="fieldsetplot">
    <div id="plot"></div>
    <div id="tooltip"></div>
    <div id="legend"></div>
  </fieldset>
  <fieldset>
    <div>
      <p>
        ----------------------------- TO-D0!------------------------------------------
        <ul>
          <li><p>The most discussed topic in both communities is about talking in relationships (id 0 with 3599 posts overall) followed by topics about
             confidence, sex, sexism, appaerance and attractiveness.</p></li>
          <li><p>The most frequently discussed topics seem to be discussed almost equally often in both subreddits as the bigger circles are colored rather grey.</p></li>
          <li><p>Althought not as clearly visible as with the BERTopic visualization, the topic seem to form certain groups or clusters or are at least closely positioned if semantically related.
            Topics in the upper right corner are about body functions and needs like using the restroom, farting or snoring and close positioned to men's and women's body topics (circumcision, period), 
            which are in turn close to appaerance related topics (hair, teeth, clothes). 
            Topic 22 about drinking and 18 about food are also neigbors, positioned right in the middle.
            At the bottom on the middle-right there are certain topics about society, like working, the government, mititary or environment.
            We have seen in the similarity matrix above, that the topics with the highest overall appearance are quite similar. 
            This is also represented in our visulaization, as these topics tend to be in the lower left side of the plot.
          </p></li>
          <li><p>For the AskWomen subreddit, there is a small tendency to discuss more appaerance and hygiene-related topics 
            (shaving, farting, dentist, clothing), lifestyle questions (last name in marriages), topics about animals, female characters in fantasy, topics related to the womens's body (period, abortion, pregnancy) and about the temperature.
            Topics that seem especially unique to the  AskWomen community are topic 11 about a women's period and topic 45 about winter-weather, which are with over 80% more likely to be discussed there.</p></li>
          <li><p>In the AskMen subreddit, the discussions are more likely to cover topics about men's body (circumcision), sport (football, basketball) military and weapons (guns). The outlier topic 47 also appears more frequently in AskMen</p></li>
        </ul>

        <p>
        The visualization of the topics and their presence in the subreddits AskMen and AskWomen above shows that the topics most discussed
        in both communities tend to be rather similar. 
        However, some rather gender-specific topics like a womens's period of if a men is circumcised or not are clearly more likely to apear in the respective subreddit.
        </p>
        <p><strong>Limitations:</strong> </p>
        <ul>
          <li><p>The in-built visualizations of BERTopic created denser and more clearly separated clusters than it can be seen in our own visualization.
            Thus, we also tested several other dimensionality reduction techniques, such as MDS and tSNE but these technique produced no 'better' results.</p></li>
          <li><p>BERTopic is sensitive to the input data. Thus, we tried to test the robustness of our extracted topics and the results by training BERTopic again with raw data. 
            The results are discussed in the following.</p></li>
        </ul>
      </p>
    </div>
  </fieldset>

  <div style="margin:0 auto;text-align:center">
    <object data="img_raw_data/top10_shares_adjusted.svg"></object>
  </div>

  <fieldset>
    <p>
      With the raw data, BERTopic yields different topics. As one can see from the barplot above, some of the topics are quite similar
      to those with the preprocessed data, e.g. <i>'orgasm_sex_sexual_porn'</i> seems to be similar to <i>'virgin_sex_orgasm_sexual'</i> and
      <i>'shave_hair_hairs_shaving'</i> to <i>'shave_shaving_hair_shaved'</i>. Whereas again many of the top 10 topics seem to be 
      discussed approximately equally often in both communities, there is quite a difference in the share when it comes to the topic
      <i>'iud_pill_uterus_periods'</i>, which is much more frequently discussed in the AskWomen subreddit. To further investigate the
      differences between the results with raw and preprocessed data, we again have a look at all 50 topics and their similarities:
    </p>
  </fieldset>

  <fieldset id="fieldsetplot2">
    <div id="plot2"></div>
    <div id="tooltip2"></div>
    <div id="legend2"></div>
  </fieldset>

  <fieldset>
    <p>
      The graph above shows the 2-dimensional representations of the topic embeddings generated by the topic model with raw input data. 
      Even though is seems quite similar to the visualization of the preprocessed data, there are some visible differences. 
      The topics that are related seem to form denser clusters than in the figure above. However, we can still spot the same semantic clusters about relationship and family, hobbies and appearance.
      <br><br>
      When using the raw data as input to BERTopic we get 10548 reddit posts that are classified as topic -1, corresponding to 44.3% of the data (44.9% AskMen and 43.4% AskWomen).
      Compared to the results with the preprocessed data, fewer posts remain unclassified now. This indicates that the BERTopic 
      model can work at least as good with raw data than with preprocessed one. <br>

    </p>
  </fieldset>
  

  <fieldset class="highlighted">
    <div>
      <h3>Conclusion</h3>
    </div>
  </fieldset>

  <fieldset>
    <div>
      <p>
        <ul>
          <li><p>Whereas most discussed topics in both communities are very <b>similar</b>, some topics are in fact more 
            likely to occur in one or the other subreddit.</p></li>
          <li><p>Most frequently discussed topic within both communities is <b>relationships and dating</b>.</p></li>
          <li><p>However, our findings are <b>limited</b> as it is only a descriptive analysis, so no causal confirmation of those
            differences is possible.</p></li>
          <li><p>Topic modelling yields nice <b>first insights</b>, but most frequent words do not necessarily help to understand what 
            a topic might be about. </p></li>
          <li><p>In addition, the model does not account for the option that a post covers multiple topics or no clear topic at all. 
            With BERTopic, these posts are simply sorted out and not further considered.</p></li>
          <li><p>Depending on the input data and the performed preprocessing, the topic model generates different topic. 
            However, all model agree on the most common topics and the general clusters of topics in the data set. </p></li>
        </ul>
      </p>
    </div>
  </fieldset>

  <fieldset class="highlighted">
    <div>
      <h3>References</h3>
    </div>
  </fieldset>

  <fieldset>
    <div>
      <p>
        Grootendorst, M. (2022). BERTopic: Neural topic modeling with a class-based TF-IDF procedure. <i>arXiv preprint arXiv:2203.05794.</i> <br>
        Jaech, A., Zayats, V., Fang, H., Ostendorf, M., Hajushirzi, H. (2015). Talking to the crowd: What do people react to in online discussions. <i>arXiv preprint arXiv:1507.02205.</i> <br>
        Khan, A., & Golab, L. (2020). Reddit Mining to Understand Gendered Movements. <i>EDBT/ICDT Workshops.</i> <br>
        Latack, K., Patel, J., Moog, D., Spencer, D., & Nguyen, B. (2021). Withdrawal method inquiries and user experiences: An analysis of content posted on 4 gendered forums on Reddit. <i>Contraception, 104(2), 170-175.</i> <br>
        Lucy, L., & Mendelsohn, J. (2018). Using sentiment induction to understand variation in gendered online communities. <i>arXiv preprint arXiv:1811.07061.</i> <br>
        Völske, M., Potthast, M., Syed, S., & Stein, B. (2017). TL;DR: Mining Reddit to Learn Automatic Summarization. In <i>Proceedings of the Workshop on New Frontiers in Summarization</i> (pp. 59–63). Association for Computational Linguistics. <br>
        Zayats, V., & Ostendorf, M. (2018). Conversation Modeling on Reddit Using a Graph-Structured LSTM. <i>Transactions of the Association for Computational Linguistics, 6, 121-132.</i>
      </p>
    </div>
  </fieldset>
  

  <script src="index.js"></script>

</body>
</html>